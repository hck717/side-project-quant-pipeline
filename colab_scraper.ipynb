{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from datetime import datetime, timezone\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from confluent_kafka import Producer, KafkaException\n",
        "import time\n",
        "\n",
        "# --- PARAMETERS CELL ---\n",
        "# Tag this cell with \"parameters\" in Jupyter/Colab metadata\n",
        "KAFKA_BROKER = \"\"  # Papermill will override this\n",
        "\n",
        "# --- CONFIG ---\n",
        "crypto_tickers = [\"BTC-USD\", \"ETH-USD\", \"SOL-USD\", \"ADA-USD\", \"XRP-USD\"]\n",
        "equity_tickers = [\"AAPL\", \"MSFT\", \"AMZN\", \"TSLA\", \"NVDA\", \"JPM\", \"XOM\", \"META\", \"GOOGL\", \"NFLX\"]\n",
        "treasury_tickers = [\"^TNX\", \"^IRX\", \"^FVX\", \"^TYX\"]\n",
        "\n",
        "crypto_topic = \"crypto_ticks\"\n",
        "equity_topic = \"equities_ticks\"\n",
        "bonds_topic = \"bonds_data\"\n",
        "\n",
        "# --- IMPORTS ---\n",
        "try:\n",
        "    import yfinance as yf\n",
        "    import json\n",
        "    from confluent_kafka import Producer, KafkaException\n",
        "    from datetime import datetime, timezone\n",
        "    import pandas as pd\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Missing dependency: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- BROKER SETUP ---\n",
        "KAFKA_BROKER = KAFKA_BROKER or os.getenv(\"KAFKA_BROKER\", \"0.tcp.ap.ngrok.io:10432\")\n",
        "print(f\"[DEBUG] KAFKA_BROKER value: '{KAFKA_BROKER}'\")\n",
        "if not KAFKA_BROKER:\n",
        "    print(\"‚ö†Ô∏è KAFKA_BROKER not provided, exiting\")\n",
        "    raise ValueError(\"KAFKA_BROKER not provided\")\n",
        "\n",
        "print(f\"üì° Connecting to Kafka broker: {KAFKA_BROKER}\")\n",
        "try:\n",
        "    producer = Producer({'bootstrap.servers': KAFKA_BROKER})\n",
        "except KafkaException as e:\n",
        "    print(f\"‚ùå Failed to connect to Kafka: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- PRODUCE FUNCTION ---\n",
        "def produce_latest(symbols, topic, interval=\"1m\", max_retries=3, retry_delay=300):\n",
        "    print(f\"=== Fetching {topic} ===\")\n",
        "    all_data = []\n",
        "    for sym in symbols:\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                df = yf.download(sym, period=\"1d\", interval=interval, threads=False)\n",
        "                if df.empty:\n",
        "                    print(f\"‚ö†Ô∏è Empty DataFrame for {sym} in {topic}, attempt {attempt + 1}/{max_retries}\")\n",
        "                    if attempt < max_retries - 1:\n",
        "                        time.sleep(retry_delay)\n",
        "                    continue\n",
        "                # Verify columns\n",
        "                expected_columns = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
        "                if not all(col in df.columns for col in expected_columns):\n",
        "                    print(f\"‚ö†Ô∏è Invalid columns for {sym} in {topic}: {df.columns}\")\n",
        "                    if attempt < max_retries - 1:\n",
        "                        time.sleep(retry_delay)\n",
        "                    continue\n",
        "                all_data.append(df)\n",
        "                print(f\"[DEBUG] Fetched data for {sym} in {topic}: {df.columns}\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Failed to download data for {sym} in {topic}: {e}, attempt {attempt + 1}/{max_retries}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(retry_delay)\n",
        "                continue\n",
        "        else:\n",
        "            print(f\"‚ùå Failed to fetch data for {sym} in {topic} after {max_retries} attempts\")\n",
        "            continue\n",
        "\n",
        "    if not all_data:\n",
        "        print(f\"‚ùå No data fetched for {topic}\")\n",
        "        return\n",
        "\n",
        "    # Combine data into a single DataFrame\n",
        "    df = pd.concat(all_data, axis=1, keys=symbols) if len(all_data) > 1 else all_data[0]\n",
        "\n",
        "    for sym in symbols:\n",
        "        try:\n",
        "            sym_df = df[sym].reset_index() if isinstance(df, pd.DataFrame) and sym in df else df.reset_index()\n",
        "        except (KeyError, AttributeError):\n",
        "            sym_df = df.reset_index()\n",
        "            print(f\"[DEBUG] Fallback to reset_index for {sym}: {sym_df.columns}\")\n",
        "\n",
        "        # Rename 'Date' to 'Datetime' if necessary\n",
        "        if \"Date\" in sym_df.columns and \"Datetime\" not in sym_df.columns:\n",
        "            sym_df = sym_df.rename(columns={\"Date\": \"Datetime\"})\n",
        "            print(f\"[DEBUG] Renamed 'Date' to 'Datetime' for {sym}\")\n",
        "\n",
        "        # Check for required columns\n",
        "        required_columns = [\"Datetime\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
        "        if not all(col in sym_df.columns for col in required_columns):\n",
        "            print(f\"‚ö†Ô∏è Missing columns in DataFrame for {sym}: {sym_df.columns}\")\n",
        "            continue\n",
        "\n",
        "        sym_df = sym_df.dropna(subset=required_columns)\n",
        "        if sym_df.empty:\n",
        "            print(f\"‚ö†Ô∏è No valid data for {sym}\")\n",
        "            continue\n",
        "\n",
        "        latest_ts = sym_df[\"Datetime\"].max()\n",
        "        latest_rows = sym_df[sym_df[\"Datetime\"] == latest_ts]\n",
        "\n",
        "        for _, row in latest_rows.iterrows():\n",
        "            msg = {\n",
        "                \"symbol\": sym,\n",
        "                \"timestamp\": row[\"Datetime\"].isoformat(),\n",
        "                \"open\": float(row[\"Open\"]),\n",
        "                \"high\": float(row[\"High\"]),\n",
        "                \"low\": float(row[\"Low\"]),\n",
        "                \"close\": float(row[\"Close\"]),\n",
        "                \"volume\": float(row[\"Volume\"]),\n",
        "                \"ingested_at\": datetime.now(timezone.utc).isoformat()\n",
        "            }\n",
        "            print(f\"[SCRAPE DEBUG] {topic} {msg}\")\n",
        "            try:\n",
        "                producer.produce(topic, json.dumps(msg).encode(\"utf-8\"))\n",
        "            except (BufferError, KafkaException) as e:\n",
        "                print(f\"‚ùå Failed to produce message for {sym}: {e}\")\n",
        "\n",
        "    producer.flush()\n",
        "\n",
        "# --- RUN SCRAPES ---\n",
        "produce_latest(crypto_tickers, crypto_topic, interval=\"1m\")\n",
        "produce_latest(equity_tickers, equity_topic, interval=\"1m\")\n",
        "produce_latest(treasury_tickers, bonds_topic, interval=\"1d\")\n",
        "\n",
        "print(\"‚úÖ All data produced to Kafka topics.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOYWhllEsQWx0x4t0dUSNjn",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
